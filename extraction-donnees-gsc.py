import streamlit as st
import pandas as pd
import datetime
import os
import pickle
import time
import io
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
import base64

# Configuration de la page Streamlit
st.set_page_config(page_title="Extracteur de donn√©es Google Search Console", layout="wide")

# Styles CSS personnalis√©s
st.markdown("""
<style>
    .main-header {
        font-size: 26px;
        font-weight: bold;
        margin-bottom: 20px;
    }
    .sub-header {
        font-size: 18px;
        font-weight: bold;
        margin-top: 15px;
        margin-bottom: 10px;
    }
    .info-text {
        font-size: 14px;
        margin-bottom: 10px;
    }
    .stButton > button {
        background-color: #4CAF50;
        color: white;
        padding: 10px 24px;
        border-radius: 8px;
        border: none;
        font-size: 16px;
        font-weight: bold;
        transition-duration: 0.3s;
    }
    .stButton > button:hover {
        background-color: #45a049;
        box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
    }
    .download-link {
        display: inline-block;
        padding: 8px 16px;
        background-color: #3498db;
        color: white !important;
        text-decoration: none;
        border-radius: 5px;
        margin-top: 10px;
        font-weight: 500;
        text-align: center;
        transition: background-color 0.3s;
    }
    .download-link:hover {
        background-color: #2980b9;
        text-decoration: none;
    }
    div[data-baseweb="radio"] > div {
        margin-bottom: 8px;
        padding: 10px;
        border-radius: 5px;
        transition: background-color 0.2s;
    }
    div[data-baseweb="radio"] > div:hover {
        background-color: #f8f9fa;
    }
    /* Style pour les succ√®s */
    .element-container .stSuccess {
        background-color: #d4edda;
        color: #155724;
        padding: 15px;
        border-radius: 5px;
        border-left: 5px solid #28a745;
    }
    /* Style pour les avertissements */
    .element-container .stWarning {
        background-color: #fff3cd;
        color: #856404;
        padding: 15px;
        border-radius: 5px;
        border-left: 5px solid #ffc107;
    }
    /* Style pour les erreurs */
    .element-container .stError {
        background-color: #f8d7da;
        color: #721c24;
        padding: 15px;
        border-radius: 5px;
        border-left: 5px solid #dc3545;
    }
    /* Style pour les messages d'acc√®s refus√© */
    .access-denied {
        background-color: #f8f9fa;
        border-left: 5px solid #6c757d;
        color: #495057;
        padding: 15px;
        border-radius: 5px;
        margin: 10px 0;
        font-size: 14px;
    }
    .access-denied strong {
        color: #495057;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# Titre de l'application
st.markdown('<div class="main-header">Extracteur de donn√©es Google Search Console</div>', unsafe_allow_html=True)

# Fonction d'authentification √† l'API Google Search Console
def authenticate_gsc():
    SCOPES = ['https://www.googleapis.com/auth/webmasters']
    creds = None
    
    # V√©rifier si nous sommes dans l'environnement Streamlit Cloud
    try:
        # Tenter d'acc√©der aux secrets Streamlit
        if 'token' in st.secrets:
            from google.oauth2.credentials import Credentials
            
            creds = Credentials(
                token=st.secrets.token.get('token', None),
                refresh_token=st.secrets.token.get('refresh_token', None),
                token_uri=st.secrets.token.get('token_uri', 'https://oauth2.googleapis.com/token'),
                client_id=st.secrets.credentials.get('client_id', None),
                client_secret=st.secrets.credentials.get('client_secret', None),
                scopes=SCOPES
            )
            
            st.success("Authentification via Streamlit Secrets r√©ussie!")
            
    except Exception as e:
        # Utiliser la m√©thode locale si les secrets ne sont pas disponibles
        st.info("Utilisation de l'authentification locale.")
        
        if os.path.exists('token.pickle'):
            with open('token.pickle', 'rb') as token:
                creds = pickle.load(token)
                
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                if not os.path.exists('credentials.json'):
                    st.error("Le fichier credentials.json n'existe pas pour l'authentification locale.")
                    st.info("Veuillez configurer les secrets Streamlit ou fournir le fichier credentials.json.")
                    st.stop()
                    
                flow = InstalledAppFlow.from_client_secrets_file(
                    'credentials.json', SCOPES)
                creds = flow.run_local_server(port=8080)
                
            with open('token.pickle', 'wb') as token:
                pickle.dump(creds, token)
    
    # Si nous n'avons toujours pas de credentials valides
    if not creds:
        st.error("Impossible d'authentifier √† l'API Google Search Console.")
        st.info("Veuillez configurer les secrets Streamlit ou utiliser l'application localement.")
        st.stop()
    
    service = build('searchconsole', 'v1', credentials=creds)
    return service

# Fonction pour r√©cup√©rer les propri√©t√©s disponibles
def get_properties(service):
    site_list = service.sites().list().execute()
    properties = []
    
    if 'siteEntry' in site_list:
        for site in site_list['siteEntry']:
            properties.append(site['siteUrl'])
    
    return properties

# Fonction pour g√©rer les erreurs d'acc√®s
def handle_access_error(error_message):
    if "User does not have sufficient permission for site" in error_message:
        property_name = error_message.split("'")[1] if "'" in error_message else "cette propri√©t√©"
        st.markdown(f"""
        <div class="access-denied">
            <strong>‚ö†Ô∏è Acc√®s non autoris√©</strong><br>
            Vous n'avez pas les droits suffisants pour acc√©der √† la propri√©t√© <strong>{property_name}</strong>.<br>
            V√©rifiez que cette propri√©t√© vous a bien √©t√© partag√©e avec les droits appropri√©s dans Google Search Console.
        </div>
        """, unsafe_allow_html=True)
        return True
    return False

# Fonction pour extraire les donn√©es par page avec contournement de la limite
def get_page_data(service, site_url, start_date, end_date):
    """
    Extrait les donn√©es de pages avec pagination pour d√©passer la limite de 25000 URLs.
    R√©cup√®re toutes les donn√©es disponibles sans limite maximale.
    """
    all_results = []
    start_row = 0
    total_fetched = 0
    row_limit = 25000  # Valeur maximale fixe pour optimiser l'extraction
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    more_data = True
    while more_data:
        progress_text.info(f"Extraction des donn√©es par pages... ({total_fetched} lignes r√©cup√©r√©es)")
        
        request = {
            'startDate': start_date,
            'endDate': end_date,
            'dimensions': ['page'],
            'rowLimit': row_limit,
            'startRow': start_row
        }
        
        try:
            response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
            
            if 'rows' not in response or not response['rows']:
                more_data = False
                break  # Pas plus de donn√©es √† r√©cup√©rer
            
            batch_size = len(response['rows'])
            for row in response['rows']:
                all_results.append({
                    'page': row['keys'][0],
                    'clicks': row['clicks'],
                    'impressions': row['impressions'],
                    'ctr': row['ctr'],
                    'position': row['position']
                })
            
            # Mise √† jour pour la prochaine it√©ration
            start_row += batch_size
            total_fetched += batch_size
            
            # Si nous avons re√ßu moins de lignes que demand√©es, nous avons fini
            if batch_size < row_limit:
                more_data = False
                
            # Mise √† jour de la barre de progression (estimation bas√©e sur le fait que nous recevons moins de donn√©es)
            progress_completion = 1.0 if not more_data else min(0.9, batch_size / row_limit)
            progress_bar.progress(progress_completion)
                
            # Pause pour √©viter de d√©passer les quotas d'API
            time.sleep(0.5)
            
        except Exception as e:
            error_message = str(e)
            if not handle_access_error(error_message):
                st.error(f"Erreur lors de l'extraction des donn√©es par pages : {e}")
            more_data = False
    
    progress_text.empty()
    progress_bar.empty()
    return pd.DataFrame(all_results)

# Fonction pour extraire les donn√©es par mot-cl√© avec contournement de la limite
def get_query_data(service, site_url, start_date, end_date):
    """
    Extrait les donn√©es de mots-cl√©s avec pagination pour d√©passer la limite de 25000.
    R√©cup√®re toutes les donn√©es disponibles sans limite maximale.
    """
    all_results = []
    start_row = 0
    total_fetched = 0
    row_limit = 25000  # Valeur maximale fixe pour optimiser l'extraction
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    more_data = True
    while more_data:
        progress_text.info(f"Extraction des donn√©es par mots-cl√©s... ({total_fetched} lignes r√©cup√©r√©es)")
        
        request = {
            'startDate': start_date,
            'endDate': end_date,
            'dimensions': ['query'],
            'rowLimit': row_limit,
            'startRow': start_row
        }
        
        try:
            response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
            
            if 'rows' not in response or not response['rows']:
                more_data = False
                break  # Pas plus de donn√©es √† r√©cup√©rer
            
            batch_size = len(response['rows'])
            for row in response['rows']:
                all_results.append({
                    'mot-cl√©': row['keys'][0],
                    'clicks': row['clicks'],
                    'impressions': row['impressions'],
                    'ctr': row['ctr'],
                    'position': row['position']
                })
            
            # Mise √† jour pour la prochaine it√©ration
            start_row += batch_size
            total_fetched += batch_size
            
            # Si nous avons re√ßu moins de lignes que demand√©es, nous avons fini
            if batch_size < row_limit:
                more_data = False
                
            # Mise √† jour de la barre de progression (estimation bas√©e sur le fait que nous recevons moins de donn√©es)
            progress_completion = 1.0 if not more_data else min(0.9, batch_size / row_limit)
            progress_bar.progress(progress_completion)
                
            # Pause pour √©viter de d√©passer les quotas d'API
            time.sleep(0.5)
            
        except Exception as e:
            error_message = str(e)
            if not handle_access_error(error_message):
                st.error(f"Erreur lors de l'extraction des donn√©es par mots-cl√©s : {e}")
            more_data = False
    
    progress_text.empty()
    progress_bar.empty()
    return pd.DataFrame(all_results)

# Fonction pour extraire les donn√©es par page et mot-cl√© avec contournement de la limite
def get_page_query_data(service, site_url, start_date, end_date):
    """
    Extrait les donn√©es de pages et mots-cl√©s avec pagination pour d√©passer la limite de 25000.
    R√©cup√®re toutes les donn√©es disponibles sans limite maximale.
    """
    all_results = []
    start_row = 0
    total_fetched = 0
    row_limit = 25000  # Valeur maximale fixe pour optimiser l'extraction
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    more_data = True
    while more_data:
        progress_text.info(f"Extraction des donn√©es par pages et mots-cl√©s... ({total_fetched} lignes r√©cup√©r√©es)")
        
        request = {
            'startDate': start_date,
            'endDate': end_date,
            'dimensions': ['page', 'query'],
            'rowLimit': row_limit,
            'startRow': start_row
        }
        
        try:
            response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
            
            if 'rows' not in response or not response['rows']:
                more_data = False
                break  # Pas plus de donn√©es √† r√©cup√©rer
            
            batch_size = len(response['rows'])
            for row in response['rows']:
                all_results.append({
                    'page': row['keys'][0],
                    'mot-cl√©': row['keys'][1],
                    'clicks': row['clicks'],
                    'impressions': row['impressions'],
                    'ctr': row['ctr'],
                    'position': row['position']
                })
            
            # Mise √† jour pour la prochaine it√©ration
            start_row += batch_size
            total_fetched += batch_size
            
            # Si nous avons re√ßu moins de lignes que demand√©es, nous avons fini
            if batch_size < row_limit:
                more_data = False
                
            # Mise √† jour de la barre de progression (estimation bas√©e sur le fait que nous recevons moins de donn√©es)
            progress_completion = 1.0 if not more_data else min(0.9, batch_size / row_limit)
            progress_bar.progress(progress_completion)
                
            # Pause pour √©viter de d√©passer les quotas d'API
            time.sleep(0.5)
            
        except Exception as e:
            error_message = str(e)
            if not handle_access_error(error_message):
                st.error(f"Erreur lors de l'extraction des donn√©es par pages et mots-cl√©s : {e}")
            more_data = False
    
    progress_text.empty()
    progress_bar.empty()
    return pd.DataFrame(all_results)

# Fonction pour g√©n√©rer un lien de t√©l√©chargement pour CSV
def get_download_link(df, filename, text):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}" class="download-link">T√©l√©charger {text}</a>'
    return href

# Fonction pour g√©n√©rer un lien de t√©l√©chargement pour Excel avec plusieurs onglets
def get_excel_download_link(dataframes, sheet_names, filename, text):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for df, sheet_name in zip(dataframes, sheet_names):
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    
    excel_data = output.getvalue()
    b64 = base64.b64encode(excel_data).decode()
    href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}" class="download-link" style="background-color: #2e7d32;">T√©l√©charger {text}</a>'
    return href

# Fonction principale
def main():
    # Tentative d'authentification
    try:
        service = authenticate_gsc()
        
        # R√©cup√©ration des propri√©t√©s
        properties = get_properties(service)
        
        if not properties:
            st.warning("Aucune propri√©t√© n'a √©t√© trouv√©e pour ce compte Google.")
            return
        
        # S√©lection de la propri√©t√©
        selected_property = st.selectbox(
            "S√©lectionnez une propri√©t√©",
            properties
        )
        
        # S√©lection de la plage de dates
        st.markdown('<div class="sub-header">S√©lectionnez la plage de dates</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            start_date = st.date_input(
                "Date de d√©but",
                value=datetime.date.today() - datetime.timedelta(days=30)
            )
        
        with col2:
            end_date = st.date_input(
                "Date de fin",
                value=datetime.date.today() - datetime.timedelta(days=1)
            )
        
        # Validation des dates
        if start_date > end_date:
            st.error("La date de d√©but doit √™tre ant√©rieure √† la date de fin.")
            return
        
        # V√©rification si la plage de dates est trop grande
        if (end_date - start_date).days > 90:
            st.warning("Attention: Une plage de dates sup√©rieure √† 90 jours peut entra√Æner des temps d'extraction plus longs.")
        
        # Conversion des dates au format requis par l'API
        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")
        
        # Options d'extraction
        st.markdown('<div class="sub-header">Options d\'extraction</div>', unsafe_allow_html=True)
        
        extraction_type = st.radio(
            "S√©lectionnez le type d'extraction :",
            ["Extraire les donn√©es par pages", 
             "Extraire les donn√©es par mots-cl√©s", 
             "Extraire les donn√©es par pages et mots-cl√©s",
             "Extraire les trois types de donn√©es"]
        )
        
        # Variables pour les options d'extraction
        extract_pages = extraction_type == "Extraire les donn√©es par pages" or extraction_type == "Extraire les trois types de donn√©es"
        extract_queries = extraction_type == "Extraire les donn√©es par mots-cl√©s" or extraction_type == "Extraire les trois types de donn√©es"
        extract_pages_queries = extraction_type == "Extraire les donn√©es par pages et mots-cl√©s" or extraction_type == "Extraire les trois types de donn√©es"
        
        # Bouton d'extraction avec style am√©lior√©
        st.markdown("<br>", unsafe_allow_html=True)  # Espace avant le bouton
        extract_button = st.button("üìä Extraire les donn√©es")
        
        if extract_button:
            with st.spinner("Extraction des donn√©es en cours..."):
                # Cr√©ation d'un conteneur pour afficher la progression
                progress_container = st.empty()
                
                # Extraction selon le type s√©lectionn√©
                if extraction_type == "Extraire les trois types de donn√©es":
                    # Cr√©er un conteneur pour afficher la progression
                    progress_container.info("Extraction des trois types de donn√©es (avec pagination)...")
                    
                    # Extraction des pages
                    st.write("1. Extraction des donn√©es par pages...")
                    pages_df = get_page_data(service, selected_property, start_date_str, end_date_str)
                    if not pages_df.empty:
                        st.success(f"Extraction des donn√©es par pages r√©ussie: {len(pages_df)} lignes.")
                        st.dataframe(pages_df.head(10))
                    else:
                        st.warning("Aucune donn√©e par page n'a √©t√© trouv√©e.")
                    
                    # Extraction des mots-cl√©s
                    st.write("2. Extraction des donn√©es par mots-cl√©s...")
                    queries_df = get_query_data(service, selected_property, start_date_str, end_date_str)
                    if not queries_df.empty:
                        st.success(f"Extraction des donn√©es par mots-cl√©s r√©ussie: {len(queries_df)} lignes.")
                        st.dataframe(queries_df.head(10))
                    else:
                        st.warning("Aucune donn√©e par mot-cl√© n'a √©t√© trouv√©e.")
                    
                    # Extraction des pages et mots-cl√©s
                    st.write("3. Extraction des donn√©es par pages et mots-cl√©s...")
                    pages_queries_df = get_page_query_data(service, selected_property, start_date_str, end_date_str)
                    if not pages_queries_df.empty:
                        st.success(f"Extraction des donn√©es par pages et mots-cl√©s r√©ussie: {len(pages_queries_df)} lignes.")
                        st.dataframe(pages_queries_df.head(10))
                    else:
                        st.warning("Aucune donn√©e par page et mot-cl√© n'a √©t√© trouv√©e.")
                    
                    # Proposer le t√©l√©chargement d'un fichier Excel avec les trois types de donn√©es
                    if not pages_df.empty or not queries_df.empty or not pages_queries_df.empty:
                        # Cr√©er des dataframes vides pour les feuilles manquantes si n√©cessaire
                        if pages_df.empty:
                            pages_df = pd.DataFrame(columns=['page', 'clicks', 'impressions', 'ctr', 'position'])
                        if queries_df.empty:
                            queries_df = pd.DataFrame(columns=['mot-cl√©', 'clicks', 'impressions', 'ctr', 'position'])
                        if pages_queries_df.empty:
                            pages_queries_df = pd.DataFrame(columns=['page', 'mot-cl√©', 'clicks', 'impressions', 'ctr', 'position'])
                        
                        dataframes = [pages_df, queries_df, pages_queries_df]
                        sheet_names = ["Pages", "Mots-cl√©s", "Pages et Mots-cl√©s"]
                        
                        date_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                        excel_filename = f"gsc_data_{date_str}.xlsx"
                        
                        st.markdown(get_excel_download_link(dataframes, sheet_names, excel_filename, 
                                                          "le fichier Excel avec les trois types de donn√©es"), 
                                  unsafe_allow_html=True)
                        
                        # Proposer aussi le t√©l√©chargement des fichiers CSV individuels
                        st.write("Vous pouvez √©galement t√©l√©charger chaque type de donn√©es s√©par√©ment en CSV :")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            if not pages_df.empty:
                                st.markdown(get_download_link(pages_df, "pages_data.csv", "les donn√©es par pages (CSV)"), 
                                          unsafe_allow_html=True)
                        with col2:
                            if not queries_df.empty:
                                st.markdown(get_download_link(queries_df, "queries_data.csv", "les donn√©es par mots-cl√©s (CSV)"), 
                                          unsafe_allow_html=True)
                        with col3:
                            if not pages_queries_df.empty:
                                st.markdown(get_download_link(pages_queries_df, "pages_queries_data.csv", 
                                                           "les donn√©es par pages et mots-cl√©s (CSV)"), 
                                          unsafe_allow_html=True)
                
                # Extraction des donn√©es par page
                elif extraction_type == "Extraire les donn√©es par pages":
                    progress_container.info("Extraction des donn√©es par pages (avec pagination)...")
                    pages_df = get_page_data(service, selected_property, start_date_str, end_date_str)
                    if not pages_df.empty:
                        st.success(f"Extraction des donn√©es par pages r√©ussie: {len(pages_df)} lignes.")
                        st.dataframe(pages_df.head(10))
                        st.markdown(get_download_link(pages_df, "pages_data.csv", "les donn√©es par pages"), unsafe_allow_html=True)
                    else:
                        st.warning("Aucune donn√©e par page n'a √©t√© trouv√©e.")
                
                # Extraction des donn√©es par mot-cl√©
                elif extraction_type == "Extraire les donn√©es par mots-cl√©s":
                    progress_container.info("Extraction des donn√©es par mots-cl√©s (avec pagination)...")
                    queries_df = get_query_data(service, selected_property, start_date_str, end_date_str)
                    if not queries_df.empty:
                        st.success(f"Extraction des donn√©es par mots-cl√©s r√©ussie: {len(queries_df)} lignes.")
                        st.dataframe(queries_df.head(10))
                        st.markdown(get_download_link(queries_df, "queries_data.csv", "les donn√©es par mots-cl√©s"), unsafe_allow_html=True)
                    else:
                        st.warning("Aucune donn√©e par mot-cl√© n'a √©t√© trouv√©e.")
                
                # Extraction des donn√©es par page et mot-cl√©
                elif extraction_type == "Extraire les donn√©es par pages et mots-cl√©s":
                    progress_container.info("Extraction des donn√©es par pages et mots-cl√©s (avec pagination)...")
                    pages_queries_df = get_page_query_data(service, selected_property, start_date_str, end_date_str)
                    if not pages_queries_df.empty:
                        st.success(f"Extraction des donn√©es par pages et mots-cl√©s r√©ussie: {len(pages_queries_df)} lignes.")
                        st.dataframe(pages_queries_df.head(10))
                        st.markdown(get_download_link(pages_queries_df, "pages_queries_data.csv", "les donn√©es par pages et mots-cl√©s"), unsafe_allow_html=True)
                    else:
                        st.warning("Aucune donn√©e par page et mot-cl√© n'a √©t√© trouv√©e.")
                
                progress_container.empty()
                
    except Exception as e:
        error_message = str(e)
        if not handle_access_error(error_message):
            if "Address already in use" in error_message:
                st.markdown(f"""
                <div class="access-denied">
                    <strong>‚ö†Ô∏è Port d√©j√† utilis√©</strong><br>
                    Un autre programme utilise d√©j√† le port n√©cessaire √† l'authentification.<br>
                    Essayez de fermer d'autres instances de Streamlit ou red√©marrez votre ordinateur.
                </div>
                """, unsafe_allow_html=True)
            else:
                st.error(f"Une erreur s'est produite: {e}")
        
        st.info("Assurez-vous que vous avez correctement configur√© les identifiants et que vous avez l'acc√®s aux propri√©t√©s Google Search Console.")

if __name__ == "__main__":
    main()
